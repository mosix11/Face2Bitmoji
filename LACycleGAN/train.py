import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras
import data.dataset as ds
import data.image_util as im_util
import model.modules as modules
import model.networks as networks
import config
import time
import datetime


@tf.function
def train_step(A_img, A_lm, A_gndr, B_img, B_lm, B_gndr, active_CGD):
    # A and B are image batches of size BTACH_SIZE

    A_img_rand_crp = im_util.random_jitter(A_img, (160,160))
    B_img_rand_crp = im_util.random_jitter(B_img, (160,160))

    with tf.GradientTape(persistent=True) as tape:

        A2B = gen_A2B([A_img, A_lm], training=True) # fake B image generated by A2B generatot
        B2A = gen_B2A([B_img, B_lm], training=True) # fake A image generated by B2A generatot
        # A2B = gen_A2B(A_img, training=True) # fake B image generated by A2B generatot
        # B2A = gen_B2A(B_img, training=True) # fake A image generated by B2A generatot
        # A2B = gen_A2B([A_img, A_gndr], training=True) # fake B image generated by A2B generatot
        # B2A = gen_B2A([B_img, B_gndr], training=True) # fake A image generated by B2A generatot

        A2B_lm = B_lm_reg(A2B)
        B2A_lm = A_lm_reg(B2A)
        
        # A2B2A = gen_B2A(A2B, training=True) # cycle
        # B2A2B = gen_A2B(B2A, training=True) # cycle
        # A2B2A = gen_B2A([A2B, A_gndr], training=True) # cycle
        # B2A2B = gen_A2B([B2A, B_gndr], training=True) # cycle
        A2B2A = gen_B2A([A2B, A2B_lm], training=True) # cycle
        B2A2B = gen_A2B([B2A, B2A_lm], training=True) # cycle
        # A2A = gen_B2A([A_img, A_lm], training=True) # identity
        # B2B = gen_A2B([B_img, B_lm], training=True) # identity


        fake_UGD_disc_B_preds = disc_UGD_B(A2B, training=True) # UGD disc B score for fake B image generated by A2B generatot
        real_UGD_disc_B_preds = disc_UGD_B(B_img, training=True) # UGD disc B score for real B image 
        fake_UGD_disc_A_preds = disc_UGD_A(B2A, training=True) # UGD disc A score for fake A image generated by B2A generatot
        real_UGD_disc_A_preds = disc_UGD_A(A_img, training=True) # UGD disc A score for real A image
        # fake_UGD_disc_B_preds = disc_UGD_B([A2B, A_gndr], training=True) # UGD disc B score for fake B image generated by A2B generator
        # real_UGD_disc_B_preds = disc_UGD_B([B_img, B_gndr], training=True) # UGD disc B score for real B image 
        # fake_UGD_disc_A_preds = disc_UGD_A([B2A, B_gndr], training=True) # UGD disc A score for fake A image generated by B2A generator
        # real_UGD_disc_A_preds = disc_UGD_A([A_img, A_gndr], training=True) # UGD disc A score for real A image

        disc_UGD_B_loss = networks.discriminator_adv_loss(real_UGD_disc_B_preds, fake_UGD_disc_B_preds)
        disc_UGD_A_loss = networks.discriminator_adv_loss(real_UGD_disc_A_preds, fake_UGD_disc_A_preds)



        fake1_CGD_disc_B_preds = disc_CGD_B([A2B, A2B_lm], training=True) # CGD disc B score for fake B image generated by A2B generatot
        fake2_CGD_disc_B_preds = disc_CGD_B([B_img_rand_crp, B_lm], training=True) # CGD disc B score for jittered B image and corresponding non jittered landmark
        real_CGD_disc_B_preds = disc_CGD_B([B_img, B_lm], training=True) # CGD disc B score for real B image 
        fake1_CGD_disc_A_preds = disc_CGD_A([B2A, B2A_lm], training=True) # CGD disc A score for fake A image generated by B2A generatot
        fake2_CGD_disc_A_preds = disc_CGD_A([A_img_rand_crp, A_lm], training=True) # CGD disc A score for jittered A image and corresponding non jittered landmark
        real_CGD_disc_A_preds = disc_CGD_A([A_img, A_lm], training=True) # CGD disc A score for real A image

        disc_CGD_B_loss = networks.discriminator_adv_loss_CGD(real_CGD_disc_B_preds, fake1_CGD_disc_B_preds, fake2_CGD_disc_B_preds)
        disc_CGD_A_loss = networks.discriminator_adv_loss_CGD(real_CGD_disc_A_preds, fake1_CGD_disc_A_preds, fake2_CGD_disc_A_preds)
        # disc_CGD_B_loss = networks.discriminator_adv_loss(real_CGD_disc_B_preds, fake1_CGD_disc_B_preds)
        # disc_CGD_A_loss = networks.discriminator_adv_loss(real_CGD_disc_A_preds, fake1_CGD_disc_A_preds)


        if active_CGD:
            A2B_adv_loss = networks.generator_adv_loss(fake_UGD_disc_B_preds) * config.LAMBDA_UGD + networks.generator_adv_loss(fake1_CGD_disc_B_preds) * config.LAMBDA_CGD
            B2A_adv_loss = networks.generator_adv_loss(fake_UGD_disc_A_preds) * config.LAMBDA_UGD + networks.generator_adv_loss(fake1_CGD_disc_A_preds) * config.LAMBDA_CGD
        else :
            A2B_adv_loss = networks.generator_adv_loss(fake_UGD_disc_B_preds)
            B2A_adv_loss = networks.generator_adv_loss(fake_UGD_disc_A_preds)



        A2B2A_cycle_loss = networks.cycle_consistency_loss(A_img, A2B2A)
        B2A2B_cycle_loss = networks.cycle_consistency_loss(B_img, B2A2B)

        # A2A_id_loss = networks.identity_loss(A_img, A2A)
        # B2B_id_loss = networks.identity_loss(B_img, B2B)

        A2B_lm_cons_loss = networks.landmark_consistency_loss(A_lm, A2B_lm)
        B2A_lm_cons_loss = networks.landmark_consistency_loss(B_lm, B2A_lm)

        # gen_loss = (A2B_adv_loss + B2A_adv_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) + (A2A_id_loss + B2B_id_loss) + (A2B_lm_cons_loss + B2A_lm_cons_loss)
        gen_loss = (A2B_adv_loss + B2A_adv_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) + (A2B_lm_cons_loss + B2A_lm_cons_loss)


    
    gen_grad = tape.gradient(gen_loss, gen_A2B.trainable_variables + gen_B2A.trainable_variables)
    gen_optimizer.apply_gradients(zip(gen_grad, gen_A2B.trainable_variables + gen_B2A.trainable_variables))

    disc_UGD_A_grad = tape.gradient(disc_UGD_A_loss, disc_UGD_A.trainable_variables)
    disc_UGD_B_grad = tape.gradient(disc_UGD_B_loss, disc_UGD_B.trainable_variables)
    disc_UGD_A_optimizer.apply_gradients(zip(disc_UGD_A_grad, disc_UGD_A.trainable_variables))
    disc_UGD_B_optimizer.apply_gradients(zip(disc_UGD_B_grad, disc_UGD_B.trainable_variables))

    if active_CGD :
        disc_CGD_A_grad = tape.gradient(disc_CGD_A_loss, disc_CGD_A.trainable_variables)
        disc_CGD_B_grad = tape.gradient(disc_CGD_B_loss, disc_CGD_B.trainable_variables)
        disc_CGD_A_optimizer.apply_gradients(zip(disc_CGD_A_grad, disc_CGD_A.trainable_variables))
        disc_CGD_B_optimizer.apply_gradients(zip(disc_CGD_B_grad, disc_CGD_B.trainable_variables))








    return {'A2B_adv': A2B_adv_loss,
            'B2A_adv': B2A_adv_loss,
            'A2B2A_cycle': A2B2A_cycle_loss,
            'B2A2B_cycle': B2A2B_cycle_loss,
            'A2A_id': 0,
            'B2B_id': 0,
            'A2B_landmark_cons': A2B_lm_cons_loss,
            'B2A_landmark_cons': B2A_lm_cons_loss,
            'gen_loss': gen_loss,
            'disc_UGD_A': disc_UGD_A_loss,
            'disc_CGD_A': disc_CGD_A_loss,
            'disc_UGD_B': disc_UGD_B_loss,
            'disc_CGD_B': disc_CGD_B_loss
            }

    # return {'A2B_adv': A2B_adv_loss,
    #         'B2A_adv': B2A_adv_loss,
    #         'A2B2A_cycle': A2B2A_cycle_loss,
    #         'B2A2B_cycle': B2A2B_cycle_loss,
    #         'A2A_id': A2A_id_loss,
    #         'B2B_id': B2B_id_loss,
    #         'A2B_landmark_cons': A2B_lm_cons_loss,
    #         'B2A_landmark_cons': B2A_lm_cons_loss,
    #         'gen_loss': gen_loss,
    #         'disc_UGD_A': disc_UGD_A_loss,
    #         'disc_CGD_A': disc_CGD_A_loss,
    #         'disc_UGD_B': disc_UGD_B_loss,
    #         'disc_CGD_B': disc_CGD_B_loss
    #         }

                    # tf.summary.scalar('disc_UGD_A', loss_dict['disc_UGD_A'], step=i)
                    # tf.summary.scalar('disc_CGD_A', loss_dict['disc_CGD_A'], step=i)
                    # tf.summary.scalar('disc_UGD_B', loss_dict['disc_UGD_B'], step=i)
                    # tf.summary.scalar('disc_CGD_B', loss_dict['disc_CGD_B'], step=i)
                    # tf.summary.scalar('A2B_adv', loss_dict['A2B_adv'], step=i)
                    # tf.summary.scalar('B2A_adv', loss_dict['B2A_adv'], step=i)
                    # tf.summary.scalar('A2B_identity', loss_dict['B2B_id'], step=i)
                    # tf.summary.scalar('B2A_identity', loss_dict['A2A_id'], step=i)
                    # tf.summary.scalar('A2B2A_cycle_loss', loss_dict['A2B2A_cycle'], step=i)
                    # tf.summary.scalar('B2A2B_cycle_loss', loss_dict['B2A2B_cycle'], step=i)
                    # tf.summary.scalar('A2B_landmark_cons', loss_dict['A2B_landmark_cons'], step=i)
                    # tf.summary.scalar('B2A_landmark_cons', loss_dict['B2A_landmark_cons'], step=i)


def fit():

    sample_A_img_const = None
    for (s, ss, sss) in train_A_dataset.take(1) :
        sample_A_img_const = [s, ss]

    with summary_writer.as_default():

        i = iter_counter.value()
        break_flag = False
        start = time.time()
        while(True):
            zipped_dataset = ds.zip_datasets(train_A_dataset, train_B_dataset)
            for b, ((A_img, A_lm, A_gndr), (B_img, B_lm, B_gndr)) in enumerate(zipped_dataset):
                loss_dict = train_step(A_img, A_lm, A_gndr, B_img, B_lm, B_gndr, i > config.ITERS)
                # loss_dict={}

                # A2B, A2B_lm, B2A, B2A_lm = train_stept(A_img, A_lm, B_img, B_lm)
                # A2B, A2B_lm, B2A, B2A_lm = A2B[0], A2B_lm[0], B2A[0], B2A_lm[0]
                # plt.subplot(2,2,1)
                # plt.imshow(im_util.normalize(B_img[0], -1, 1, 0, 1))
                # plt.subplot(2,2,2)
                # plt.imshow(im_util.normalize(B_lm[0], -1, 1, 0, 1))
                # plt.subplot(2,2,3)
                # plt.imshow(im_util.normalize(B_lm_reg(B_img)[0], -1, 1, 0, 1))
                # plt.subplot(2,2,4)
                # plt.imshow(A_img[0])
                # plt.show()


                iter_counter.assign_add(1)
                i+=1

                if i % config.ITERS_INTERVAL_FOR_SUMMARY_LOG == 0:
                    tf.summary.scalar('disc_UGD_A', loss_dict['disc_UGD_A'], step=i)
                    tf.summary.scalar('disc_CGD_A', loss_dict['disc_CGD_A'], step=i)
                    tf.summary.scalar('disc_UGD_B', loss_dict['disc_UGD_B'], step=i)
                    tf.summary.scalar('disc_CGD_B', loss_dict['disc_CGD_B'], step=i)
                    tf.summary.scalar('A2B_adv', loss_dict['A2B_adv'], step=i)
                    tf.summary.scalar('B2A_adv', loss_dict['B2A_adv'], step=i)
                    tf.summary.scalar('A2B_identity', loss_dict['B2B_id'], step=i)
                    tf.summary.scalar('B2A_identity', loss_dict['A2A_id'], step=i)
                    tf.summary.scalar('A2B2A_cycle_loss', loss_dict['A2B2A_cycle'], step=i)
                    tf.summary.scalar('B2A2B_cycle_loss', loss_dict['B2A2B_cycle'], step=i)
                    tf.summary.scalar('A2B_landmark_cons', loss_dict['A2B_landmark_cons'], step=i)
                    tf.summary.scalar('B2A_landmark_cons', loss_dict['B2A_landmark_cons'], step=i)
                    tf.summary.scalar('gen_loss', loss_dict['gen_loss'], step=i)
                    # tf.summary.scalar('learning_rate', gen_lr_scheduler.current_learning_rate, step=i)



                if i % config.ITERS_INTERVAL_FOR_SAMPLE_GENERATION == 0:
                    im_util.save_image(sample_A_img_const[0], gen_A2B(sample_A_img_const, training=True), i)
                    sample_A_img_new = None
                    for (s, ss, sss) in train_A_dataset.take(1) :
                        sample_A_img_new = [s, ss]
                    im_util.save_image(sample_A_img_new[0], gen_A2B(sample_A_img_new, training=True), i+1)

                
                if i % config.ITERS_INTERVAL_FOR_CHECKPOINT == 0:
                    ckpt_save_path = ckpt_manager.save()
                    print('Time taken for {} Iteration is {} sec and Checkpoint Saved\n'.format(i, time.time()-start))
                    start = time.time()

 
                if i == config.ITERS:
                    print('*************** OPTIMIZERS RESTORED ***************')
                    global gen_lr_scheduler, disc_UGD_A_lr_scheduler, disc_UGD_B_lr_scheduler, gen_optimizer, disc_UGD_A_optimizer, disc_UGD_B_optimizer
                    gen_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)

                    disc_UGD_A_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)          
                    disc_UGD_B_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)

                    gen_optimizer = keras.optimizers.Adam(gen_lr_scheduler, beta_1=0.5)
                    disc_UGD_A_optimizer = keras.optimizers.Adam(disc_UGD_A_lr_scheduler, beta_1=0.5)
                    disc_UGD_B_optimizer = keras.optimizers.Adam(disc_UGD_B_lr_scheduler, beta_1=0.5)


                if i == config.ITERS * 2:
                    break_flag = True
                    break

            if break_flag:
                break




train_A_dataset, train_B_dataset = ds.load_train_dataset()


disc_UGD_A = networks.UnconditionalGlobalDiscriminator()
disc_CGD_A = networks.ConditionalGlobalDiscriminator()
gen_B2A = networks.Generator()
A_lm_reg = keras.models.load_model(config.A_LANDMARK_REGRESSOR_PATH)


disc_UGD_B = networks.UnconditionalGlobalDiscriminator()
disc_CGD_B = networks.ConditionalGlobalDiscriminator()
gen_A2B = networks.Generator()
B_lm_reg = keras.models.load_model(config.B_LANDMARK_REGRESSOR_PATH)


gen_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)

disc_UGD_A_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)
disc_CGD_A_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)

disc_UGD_B_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)
disc_CGD_B_lr_scheduler = keras.optimizers.schedules.PolynomialDecay(config.START_LR, config.ITERS, config.END_LR, power=config.DECAY_POWER)

# gen_lr_scheduler = modules.LinearDecay(2e-4, config.ITERS, config.ITERS_DECAY_START)

# disc_UGD_A_lr_scheduler = modules.LinearDecay(2e-4, config.ITERS, config.ITERS_DECAY_START)
# disc_CGD_A_lr_scheduler = modules.LinearDecay(2e-4, config.ITERS, config.ITERS_DECAY_START)

# disc_UGD_B_lr_scheduler = modules.LinearDecay(2e-4, config.ITERS, config.ITERS_DECAY_START)
# disc_CGD_B_lr_scheduler = modules.LinearDecay(2e-4, config.ITERS, config.ITERS_DECAY_START)


gen_optimizer = keras.optimizers.Adam(gen_lr_scheduler, beta_1=0.5)


disc_UGD_A_optimizer = keras.optimizers.Adam(disc_UGD_A_lr_scheduler, beta_1=0.5)
disc_CGD_A_optimizer = keras.optimizers.Adam(disc_CGD_A_lr_scheduler, beta_1=0.5)

disc_UGD_B_optimizer = keras.optimizers.Adam(disc_UGD_B_lr_scheduler, beta_1=0.5)
disc_CGD_B_optimizer = keras.optimizers.Adam(disc_CGD_B_lr_scheduler, beta_1=0.5)


iter_counter = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)

ckpt = tf.train.Checkpoint(gen_A2B=gen_A2B,
                        gen_B2A=gen_B2A,
                        disc_UGD_A=disc_UGD_A,
                        disc_CGD_A=disc_CGD_A,
                        disc_UGD_B=disc_UGD_B,
                        disc_CGD_B=disc_CGD_B,
                        gen_optimizer=gen_optimizer,    
                        disc_UGD_A_optimizer=disc_UGD_A_optimizer,
                        disc_CGD_A_optimizer=disc_CGD_A_optimizer,
                        disc_UGD_B_optimizer=disc_UGD_B_optimizer,
                        disc_CGD_B_optimizer=disc_CGD_B_optimizer,
                        iter_counter=iter_counter)


ckpt_manager = tf.train.CheckpointManager(ckpt, config.CHECKPOINT_PATH, max_to_keep=20)

if ckpt_manager.latest_checkpoint:
    ckpt.restore('ckpt-95.index')
    print ('Latest checkpoint restored!!')

summary_writer = tf.summary.create_file_writer(config.LOG_DIR + "fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

fit()

